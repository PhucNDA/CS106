{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yhSyhfEy4XSD"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kHf1dAVKAcZm"
   },
   "outputs": [],
   "source": [
    "env1 = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VWLnvY7VBvIZ"
   },
   "outputs": [],
   "source": [
    "def play(env, policy, render=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(0.2)\n",
    "            if not done:\n",
    "                display.clear_output(wait=True)\n",
    "        state = next_state\n",
    "\n",
    "    return (total_reward, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JU8Q1qMxD6Po"
   },
   "outputs": [],
   "source": [
    "def play_multiple_times(env, policy, max_episodes):\n",
    "    success = 0\n",
    "    list_of_steps = []\n",
    "    for i in range(max_episodes):\n",
    "        total_reward, steps = play(env, policy)\n",
    "\n",
    "        if total_reward > 0:\n",
    "            success += 1\n",
    "            list_of_steps.append(steps)\n",
    "\n",
    "    print(f'Number of successes: {success}/{max_episodes}')\n",
    "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bSomNpxJE5lP"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
    "    # Initialize the values of all states to be 0\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        prev_v_values = np.copy(v_values)\n",
    "\n",
    "        # Update the value of each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "\n",
    "            # Compute the q-value of the action\n",
    "            q_value = 0\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "\n",
    "            v_values[state] = q_value # update v-value\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.all(np.isclose(v_values, prev_v_values)):\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "    \n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uh4akjMSHJBF"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iters=500, gamma=0.9):\n",
    "    # initialize\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iters): \n",
    "        prev_v_values = np.copy(v_values)\n",
    "\n",
    "        # update the v-value for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_values = []\n",
    "            \n",
    "            # compute the q-value for each action that we can perform at the state\n",
    "            for action in range(env.action_space.n):\n",
    "                q_value = 0\n",
    "                # loop through each possible outcome\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "                \n",
    "                q_values.append(q_value)\n",
    "            \n",
    "            # select the max q-values\n",
    "            best_action = np.argmax(q_values)\n",
    "            v_values[state] = q_values[best_action]\n",
    "        \n",
    "        # check convergence\n",
    "        if np.all(np.isclose(v_values, prev_v_values)):\n",
    "            print(f'Converged at {i}-th iteration.')\n",
    "            break\n",
    "    \n",
    "    return v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jb0an7gaV39e"
   },
   "outputs": [],
   "source": [
    "def policy_extraction(env, v_values, gamma=0.9):\n",
    "    # initialize\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # loop through each state in the environment\n",
    "    for state in range(env.observation_space.n):\n",
    "        q_values = []\n",
    "        # loop through each action\n",
    "        for action in range(env.action_space.n):\n",
    "            q_value = 0\n",
    "            # loop each possible outcome\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_value += prob * (reward + gamma * v_values[next_state])\n",
    "            \n",
    "            q_values.append(q_value)\n",
    "        \n",
    "        # select the best action\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[state] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-m4ZqWZXKqG",
    "outputId": "e142ffc5-0bdf-4a31-ae47-01f8293f52c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 79-th iteration.\n",
      "Time:  1.0349922180175781\n",
      "Number of successes: 741/1000\n",
      "Average number of steps: 37.465587044534416\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "optimal_v_values1=value_iteration(env1)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "optimal_policy1=policy_extraction(env1, optimal_v_values1, gamma=0.9)\n",
    "play_multiple_times(env1, optimal_policy1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wcYV5xbSZAHe"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iters=500, gamma=0.9):\n",
    "    # Khoi tao v_values va bang policy\n",
    "    v_values = np.zeros(env.observation_space.n)\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    # Iterate cho den khi nao la du nhieu thi dung\n",
    "    for i in range(max_iters): \n",
    "        #Tao ban copy\n",
    "        prev_policy=np.copy(policy)\n",
    "        # Converge Vpi\n",
    "        for j in range(max_iters):\n",
    "            prev_v_values = np.copy(v_values)\n",
    "            for state in range(env.observation_space.n):\n",
    "\n",
    "                # Tuan theo chien luoc pi\n",
    "                action = policy[state]\n",
    "                v_value = 0\n",
    "\n",
    "                # Loop moi action trong state s\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    v_value += prob * (reward + gamma * prev_v_values[next_state])\n",
    "\n",
    "                v_values[state]=v_value\n",
    "            # Kiem tra converge Vpi\n",
    "            if np.all(np.isclose(v_values, prev_v_values)):\n",
    "                break\n",
    "        \n",
    "        #Improvement chien luoc pi\n",
    "        for state in range(env.observation_space.n):\n",
    "            values=[]\n",
    "            for action in range(env.action_space.n):\n",
    "                v_value = 0\n",
    "                # Loop moi action trong state s\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    v_value += prob * (reward + gamma * v_values[next_state])\n",
    "                values.append(v_value)\n",
    "            policy[state]=np.argmax(values)\n",
    "        # Kiem tra policy converge    \n",
    "        if (prev_policy==policy).all():\n",
    "            break\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.0280559062957764\n",
      "Number of successes: 750/1000\n",
      "Average number of steps: 37.32\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "policy_iter1=policy_iteration(env1)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "play_multiple_times(env1, policy_iter1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('FrozenLake8x8-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 117-th iteration.\n",
      "Time:  1.190941572189331\n",
      "Number of successes: 744/1000\n",
      "Average number of steps: 72.78494623655914\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "optimal_v_values2=value_iteration(env2)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "optimal_policy2=policy_extraction(env2, optimal_v_values2, gamma=0.9)\n",
    "play_multiple_times(env2, optimal_policy2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.1691973209381104\n",
      "Number of successes: 723/1000\n",
      "Average number of steps: 70.65421853388658\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "policy_iter2=policy_iteration(env2)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "play_multiple_times(env2, policy_iter2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env3 = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at 116-th iteration.\n",
      "Time:  2.4231574535369873\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 13.011\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "optimal_v_values3=value_iteration(env3)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "optimal_policy3=policy_extraction(env3, optimal_v_values3, gamma=0.9)\n",
    "play_multiple_times(env3, optimal_policy3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  1.6230072975158691\n",
      "Number of successes: 1000/1000\n",
      "Average number of steps: 13.028\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "policy_iter3=policy_iteration(env3)\n",
    "end=time.time()\n",
    "print('Time: ',end-start + 1)\n",
    "play_multiple_times(env3, policy_iter3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét:\n",
    "\n",
    "Độ chính xác của Value Iteration và Policy Iteration là như nhau vì nó cũng sẽ converge về optimal solution, tùy thuộc vào số iteration\n",
    "\n",
    "Tuy nhiên, Policy Iteration sẽ chạy nhanh hơn Value Iteration nhưng có số vòng lặp nhiều hơn vì sẽ Policy sẽ phải converge về giá trị kì vọng nhận được khi tuân theo chiến lược Pi và sau đó sẽ phải improve chiến lược Pi để chiến lược hội tụ về Pi*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Value_Iteration_Gym.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
